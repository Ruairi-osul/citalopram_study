{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ephys_queries import (\n",
    "    select_spike_times\n",
    ")\n",
    "from ephys_queries import db_setup_core\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spiketimes.df.correlate import cross_corr_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\".\").absolute().parent / \"data\"\n",
    "group_names = (\"chronic_citalopram\", \n",
    "                           \"chronic_saline\", \n",
    "                           \"chronic_saline_\", \n",
    "                           \"citalopram_continuation\", \n",
    "                           \"citalopram_discontinuation\")\n",
    "dfb = (\n",
    "    pd.read_csv(data_dir / \"baseline.csv\")\n",
    "    .assign(group= \n",
    "            lambda x: x[\"group_name\"].map({\"chronic_saline\": \"CS\",\n",
    "                                          \"chronic_saline_\": \"CS\",\n",
    "                                          \"citalopram_continuation\": \"CC\",\n",
    "                                          \"chronic_citalopram\": \"CC\",\n",
    "                                          \"citalopram_discontinuation\": \"CD\"})\n",
    "           )\n",
    "    .loc[lambda x: x.group_name.isin(group_names)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "engine, metadata = db_setup_core()\n",
    "block_name = \"pre\"\n",
    "fs = 30000\n",
    "\n",
    "df_spiketimes = (\n",
    "    select_spike_times(\n",
    "        engine, metadata, \n",
    "        block_name=block_name, \n",
    "        group_names=group_names,\n",
    "    )\n",
    "    .assign(\n",
    "        spiketimes= lambda x: x[\"spike_time_samples\"].divide(fs)\n",
    "    )\n",
    "    .drop(\"spike_time_samples\", axis=1)\n",
    "    .merge(dfb[[\"cluster\", \"neuron_id\", \"session_name\", \"group_name\"]])\n",
    "    .loc[lambda x: x[\"cluster\"] != \"no_baseline\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_names = df_spiketimes[\"session_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0%\n",
      "50.0%\n"
     ]
    }
   ],
   "source": [
    "big_frames = []\n",
    "BINS = np.array([0.001, 0.01])\n",
    "NUM_LAGS=200\n",
    "\n",
    "for i, BIN in enumerate(BINS):\n",
    "    print(f\"{(i/len(BINS) * 100)}%\") \n",
    "    frames = []\n",
    "    for session in session_names:\n",
    "        frames.append(\n",
    "            df_spiketimes\n",
    "            .loc[lambda x: x[\"session_name\"]==session]\n",
    "            .pipe(\n",
    "                lambda x: cross_corr_test(\n",
    "                    x,\n",
    "                    binsize=BIN,\n",
    "                    num_lags=NUM_LAGS,\n",
    "                    spiketimes_col=\"spiketimes\", \n",
    "                    spiketrain_col=\"neuron_id\",\n",
    "                    use_multiprocessing=True,\n",
    "                    max_cores=10\n",
    "                )\n",
    "            )\n",
    "            .assign(session_name=session,\n",
    "                   num_lags=NUM_LAGS)\n",
    "        )\n",
    "    big_frames.append(\n",
    "        pd.concat(frames).assign(binsize=BIN)\n",
    "    )\n",
    "df = pd.concat(big_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_done = (\n",
    "    df\n",
    "    .merge(dfb[[\"neuron_id\", \"cluster\"]], left_on=\"spiketrain_1\", right_on=\"neuron_id\")\n",
    "    .drop(\"neuron_id\", axis=1)\n",
    "    .rename(columns={\"cluster\": \"spiketrain_1_cluster\"})\n",
    "    .merge(dfb[[\"neuron_id\", \"cluster\"]], left_on=\"spiketrain_2\", right_on=\"neuron_id\")\n",
    "    .drop(\"neuron_id\", axis=1)\n",
    "    .rename(columns={\"cluster\": \"spiketrain_2_cluster\"})\n",
    "    .assign(has_sr=lambda x: \n",
    "                x.apply(lambda y: (y.spiketrain_1_cluster == \"slow_regular\") or (y.spiketrain_2_cluster== \"slow_regular\"),\n",
    "                       axis=1),\n",
    "            has_sir=lambda x: \n",
    "                x.apply(lambda y: (y.spiketrain_1_cluster == \"slow_irregular\") or (y.spiketrain_2_cluster== \"slow_irregular\"),\n",
    "                       axis=1),\n",
    "            has_ff=lambda x: \n",
    "                x.apply(lambda y: (y.spiketrain_1_cluster == \"fast_firing\") or (y.spiketrain_2_cluster== \"fast_firing\"),\n",
    "                       axis=1)\n",
    "           )\n",
    "    .assign(comb= lambda x: x.apply(lambda y: \n",
    "                                    \"sr_sr\" if y.has_sr and (not y.has_sir) and (not y.has_ff)\n",
    "                                   else \"sr_sir\" if y.has_sr and y.has_sir and (not y.has_ff)\n",
    "                                   else \"sr_ff\" if y.has_sr and (not y.has_sir) and y.has_ff\n",
    "                                   else \"sir_sir\" if (not y.has_sr) and y.has_sir and (not y.has_ff)\n",
    "                                   else \"sir_ff\" if (not y.has_sr) and y.has_sir and y.has_ff\n",
    "                                   else \"ff_ff\", axis=1\n",
    "                                   ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_done.to_csv(data_dir / \"cross_corr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (\n",
    "    df_done\n",
    "    .loc[lambda x: abs(x.time_bin) > 0.001]\n",
    "    .groupby([\"spiketrain_1\", \"spiketrain_2\", \"binsize\"])[\"p\"]\n",
    "    .idxmin()\n",
    ").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_done\n",
    "    .loc[lambda x: abs(x.time_bin) > 0.001]\n",
    "    .groupby([\"spiketrain_1\", \"spiketrain_2\", \"binsize\"])[\"p\"]\n",
    "    .idxmin()\n",
    "    .reset_index()\n",
    "    .drop(\"p\", axis=1)\n",
    "    .assign(time_bin=df_done.iloc[idx].time_bin.values,\n",
    "           lowest_p=df_done.iloc[idx].p.values)\n",
    "    .merge(dfb[[\"neuron_id\", \"cluster\"]], left_on=\"spiketrain_1\", right_on=\"neuron_id\")\n",
    "    .drop(\"neuron_id\", axis=1)\n",
    "    .rename(columns={\"cluster\": \"spiketrain_1_cluster\"})\n",
    "    .merge(dfb[[\"neuron_id\", \"cluster\", \"group\"]], left_on=\"spiketrain_2\", right_on=\"neuron_id\")\n",
    "    .drop(\"neuron_id\", axis=1)\n",
    "    .rename(columns={\"cluster\": \"spiketrain_2_cluster\"})\n",
    "    .assign(has_sr=lambda x: \n",
    "                x.apply(lambda y: (y.spiketrain_1_cluster == \"slow_regular\") or (y.spiketrain_2_cluster== \"slow_regular\"),\n",
    "                       axis=1),\n",
    "            has_sir=lambda x: \n",
    "                x.apply(lambda y: (y.spiketrain_1_cluster == \"slow_irregular\") or (y.spiketrain_2_cluster== \"slow_irregular\"),\n",
    "                       axis=1),\n",
    "            has_ff=lambda x: \n",
    "                x.apply(lambda y: (y.spiketrain_1_cluster == \"fast_firing\") or (y.spiketrain_2_cluster== \"fast_firing\"),\n",
    "                       axis=1)\n",
    "           )\n",
    "    .assign(comb= lambda x: x.apply(lambda y: \n",
    "                                    \"sr_sr\" if y.has_sr and (not y.has_sir) and (not y.has_ff)\n",
    "                                   else \"sr_sir\" if y.has_sr and y.has_sir and (not y.has_ff)\n",
    "                                   else \"sr_ff\" if y.has_sr and (not y.has_sir) and y.has_ff\n",
    "                                   else \"sir_sir\" if (not y.has_sr) and y.has_sir and (not y.has_ff)\n",
    "                                   else \"sir_ff\" if (not y.has_sr) and y.has_sir and y.has_ff\n",
    "                                   else \"ff_ff\", axis=1\n",
    "                                   ))\n",
    "    .to_csv(data_dir / \"cross_corr_simp.csv\", index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
